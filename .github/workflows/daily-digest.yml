name: AI Daily Digest

on:
  schedule:
    # Run daily at UTC 00:00 (Beijing Time 08:00)
    - cron: "0 0 * * *"
  workflow_dispatch:
    inputs:
      dry:
        description: "Dry run - don't actually send email"
        default: false
        type: boolean
      github_limit:
        description: "Number of GitHub repos to include"
        default: "5"
        type: string
      hf_models_limit:
        description: "Number of HF models to include"
        default: "5"
        type: string
      arxiv_limit:
        description: "Number of arXiv papers to include"
        default: "5"
        type: string

jobs:
  daily_digest:
    runs-on: ubuntu-latest
    environment: production

    steps:
    - name: Check out repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'

    - name: Install uv
      uses: astral-sh/setup-uv@v4
      with:
        enable-cache: false

    - name: Install dependencies
      run: |
        uv sync --no-cache --extra twitter --extra youtube

    - name: Run AI Daily Digest
      env:
        # LLM Configuration
        LLM_PROVIDER: ${{ secrets.LLM_PROVIDER || 'deepseek' }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
        DEEPSEEK_API_BASE: ${{ secrets.DEEPSEEK_API_BASE }}
        # Email Configuration - SMTP (QQ Mail, Gmail, etc.)
        SMTP_HOST: ${{ secrets.SMTP_HOST }}
        SMTP_PORT: ${{ secrets.SMTP_PORT }}
        SMTP_USER: ${{ secrets.SMTP_USER }}
        SMTP_PASS: ${{ secrets.SMTP_PASS }}
        FROM_EMAIL: ${{ secrets.FROM_EMAIL }}
        TO_EMAIL: ${{ secrets.TO_EMAIL }}
        # Alternative: SendGrid
        SENDGRID_API_KEY: ${{ secrets.SENDGRID_API_KEY }}
        # Firebase (for arXiv paper tracking)
        FIREBASE_CREDENTIALS: ${{ secrets.FIREBASE_CREDENTIALS }}
        # Web integration for favorites/notes
        DIGEST_WEB_URL: ${{ secrets.DIGEST_WEB_URL }}
        SECRET_KEY: ${{ secrets.SECRET_KEY }}
        # Twitter/X API (new content source)
        TWITTER_BEARER_TOKEN: ${{ secrets.TWITTER_BEARER_TOKEN }}
        TWITTER_MIN_LIKES: ${{ secrets.TWITTER_MIN_LIKES || '100' }}
        TWITTER_MAX_TWEETS_PER_USER: ${{ secrets.TWITTER_MAX_TWEETS_PER_USER || '5' }}
        # YouTube Data API (new content source)
        YOUTUBE_API_KEY: ${{ secrets.YOUTUBE_API_KEY }}
        YOUTUBE_MIN_VIEWS: ${{ secrets.YOUTUBE_MIN_VIEWS || '10000' }}
        YOUTUBE_MIN_DURATION_MINUTES: ${{ secrets.YOUTUBE_MIN_DURATION_MINUTES || '5' }}
        YOUTUBE_MAX_VIDEOS_PER_CHANNEL: ${{ secrets.YOUTUBE_MAX_VIDEOS_PER_CHANNEL || '3' }}
        # Content source selection
        CONTENT_SOURCES: ${{ secrets.CONTENT_SOURCES || 'arxiv,blog' }}
        # Notion Output (optional)
        NOTION_TOKEN: ${{ secrets.NOTION_TOKEN }}
        NOTION_DATABASE_ID: ${{ secrets.NOTION_DATABASE_ID }}
        OUTPUT_NOTION: ${{ secrets.OUTPUT_NOTION || 'false' }}
      run: |
        echo "Running AI Daily Digest..."
        echo "LLM Provider: $LLM_PROVIDER"

        # Build command arguments
        ARGS=""
        if [ "${{ inputs.dry }}" == "true" ]; then
          ARGS="$ARGS --dry"
        fi
        if [ "${{ inputs.github_limit }}" != "" ]; then
          ARGS="$ARGS --github-limit ${{ inputs.github_limit }}"
        fi
        if [ "${{ inputs.hf_models_limit }}" != "" ]; then
          ARGS="$ARGS --hf-models-limit ${{ inputs.hf_models_limit }}"
        fi
        if [ "${{ inputs.arxiv_limit }}" != "" ]; then
          ARGS="$ARGS --arxiv-limit ${{ inputs.arxiv_limit }}"
        fi

        echo "Running: uv run arxiv-sanity-bot daily-digest $ARGS"
        uv run arxiv-sanity-bot daily-digest $ARGS
